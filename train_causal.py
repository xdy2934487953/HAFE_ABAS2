"""
Causal-HAFEè®­ç»ƒè„šæœ¬
å®ç°åŸºäºå› æœæ¨æ–­çš„ABSAæ¨¡å‹è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import argparse
import os
import sys
import numpy as np

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append('./src')

from data_loader import ABSADataset, download_datasets
from graph_builder import ABSAGraphBuilder
from causal_hafe import CausalHAFE_Model, CausalHAFE_Baseline
from evaluator import ABSAEvaluator
from utils import ExperimentLogger, create_experiment_logger
import time


def train_epoch_causal(model, graphs, frequency_buckets, dataset, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch (Causal-HAFE)"""
    model.train()
    total_loss = 0
    loss_breakdown = {'total': 0, 'task': 0, 'indep': 0, 'bias': 0, 'ib': 0}
    num_samples = 0

    for graph in graphs:
        if graph['labels'].shape[0] == 0:
            continue

        features = graph['features'].to(device)
        edge_index = graph['edge_index'].to(device)
        edge_types = graph.get('edge_types', None)
        if edge_types is not None:
            edge_types = edge_types.to(device)
        aspect_indices = graph['aspect_indices'].to(device)
        labels = graph['labels'].to(device)

        # è·å–é¢‘ç‡æ ‡ç­¾
        aspect_words = graph['aspect_words']
        frequency_labels = []
        for aspect_word in aspect_words:
            aspect_key = aspect_word.lower()
            freq_label = frequency_buckets.get(aspect_key, 2)  # é»˜è®¤ä¸­é¢‘
            frequency_labels.append(freq_label)
        frequency_labels = torch.tensor(frequency_labels, dtype=torch.long).to(device)

        optimizer.zero_grad()

        try:
            # å‰å‘ä¼ æ’­ (è¿”å›logitså’ŒDIBæŸå¤±)
            logits, dib_losses = model(
                features, edge_index, aspect_indices, edge_types,
                frequency_labels, return_dib_losses=True
            )

            # æ£€æŸ¥nan
            if torch.isnan(logits).any():
                print(f"è­¦å‘Š: logitsåŒ…å«nanï¼Œè·³è¿‡è¯¥batch")
                print(f"  featuresèŒƒå›´: [{features.min().item():.4f}, {features.max().item():.4f}]")
                print(f"  labels: {labels.tolist()}")
                print(f"  frequency_labels: {frequency_labels.tolist()}")
                continue

            # è®¡ç®—æ€»æŸå¤±
            loss, loss_dict = model.compute_total_loss(logits, labels, dib_losses)

            # æ£€æŸ¥nan
            if torch.isnan(loss):
                print(f"è­¦å‘Š: lossä¸ºnanï¼Œè·³è¿‡è¯¥batch")
                print(f"  logitsèŒƒå›´: [{logits.min().item():.4f}, {logits.max().item():.4f}]")
                print(f"  DIB losses: {dib_losses}")
                print(f"  loss_dict: {loss_dict}")
                continue

            # æ£€æŸ¥æ¢¯åº¦çˆ†ç‚¸å‰çš„å€¼
            if torch.isinf(loss):
                print(f"è­¦å‘Š: lossä¸ºinfï¼Œè·³è¿‡è¯¥batch")
                print(f"  loss_dict: {loss_dict}")
                continue

            loss.backward()

            # æ›´å¼ºçš„æ¢¯åº¦å‰ªåˆ‡ä»¥é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                print(f"è­¦å‘Š: æ¢¯åº¦åŒ…å«nan/inf (norm={grad_norm}), è·³è¿‡è¯¥batch")
                optimizer.zero_grad()
                continue

            optimizer.step()

            total_loss += loss.item()
            for key in loss_breakdown.keys():
                loss_breakdown[key] += loss_dict[key]
            num_samples += 1

        except Exception as e:
            print(f"è®­ç»ƒé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            continue

    # å¹³å‡æŸå¤±
    if num_samples == 0:
        print("è­¦å‘Š: æ²¡æœ‰æˆåŠŸè®­ç»ƒä»»ä½•æ ·æœ¬ï¼")
        return float('nan'), loss_breakdown

    avg_loss = total_loss / num_samples
    for key in loss_breakdown.keys():
        loss_breakdown[key] /= num_samples

    return avg_loss, loss_breakdown


def evaluate_causal(model, graphs, evaluator, aspect_freq, device, use_tie=False, tie_mode='basic'):
    """è¯„ä¼°æ¨¡å‹ (Causal-HAFE)"""
    model.eval()
    evaluator.reset()

    with torch.no_grad():
        for graph in graphs:
            if graph['labels'].shape[0] == 0:
                continue

            features = graph['features'].to(device)
            edge_index = graph['edge_index'].to(device)
            edge_types = graph.get('edge_types', None)
            if edge_types is not None:
                edge_types = edge_types.to(device)
            aspect_indices = graph['aspect_indices'].to(device)
            labels = graph['labels'].to(device)

            try:
                if use_tie:
                    # ä½¿ç”¨åäº‹å®æ¨ç†
                    preds, _ = model.predict_with_tie(
                        features, edge_index, aspect_indices, edge_types, tie_mode
                    )
                else:
                    # æ ‡å‡†å‰å‘ä¼ æ’­
                    logits = model(features, edge_index, aspect_indices, edge_types)
                    preds = torch.argmax(logits, dim=1)

                aspects = graph['aspect_words']
                evaluator.add_batch(preds, labels, aspects)

            except Exception as e:
                print(f"è¯„ä¼°é”™è¯¯: {e}")
                continue

    metrics = evaluator.compute_metrics(aspect_freq)
    return metrics


def main(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºå®éªŒæ—¥å¿—è®°å½•å™¨
    experiment_name = f"causal_hafe_{args.dataset}_lr{args.lr}_h{args.hidden_dim}"
    logger = create_experiment_logger(
        experiment_name,
        config=vars(args)
    )
    print(f"å®éªŒæ—¥å¿—ç›®å½•: {logger.experiment_dir}")

    start_time = time.time()

    # 1. æ£€æŸ¥æ•°æ®é›†
    if args.dataset == 'semeval2014':
        data_path = './data/semeval2014/'
        train_file = 'Restaurants_Train.xml'
    elif args.dataset == 'semeval2016':
        data_path = './data/semeval2016/'
        train_file = 'ABSA16_Restaurants_Train_SB1_v2.xml'
    else:
        raise ValueError(f"Unknown dataset: {args.dataset}")

    if not os.path.exists(os.path.join(data_path, train_file)):
        print(f"æ•°æ®é›†æœªæ‰¾åˆ°: {args.dataset}")
        download_datasets()
        return

    # 2. åŠ è½½æ•°æ®
    print(f"\n=== åŠ è½½æ•°æ®é›†: {args.dataset} ===")
    train_dataset = ABSADataset(data_path, dataset_name=args.dataset, phase='train')
    test_dataset = ABSADataset(data_path, dataset_name=args.dataset, phase='test')

    # è®¡ç®—é¢‘ç‡åˆ†æ¡¶
    frequency_buckets = train_dataset.compute_frequency_buckets(num_buckets=args.num_frequency_buckets)
    print(f"\né¢‘ç‡åˆ†æ¡¶å®Œæˆ (å…±{args.num_frequency_buckets}ä¸ªæ¡¶)")

    # 3. æ„å»ºå›¾
    print("\n=== æ„å»ºABSAå›¾ ===")
    graph_builder = ABSAGraphBuilder(device=device)

    print("å¤„ç†è®­ç»ƒé›†...")
    train_graphs = []
    skipped = 0
    for i, sample in enumerate(train_dataset):
        try:
            graph = graph_builder.build_graph(sample['text'], sample['aspects'])
            train_graphs.append(graph)
        except Exception as e:
            skipped += 1
            if skipped <= 3:
                print(f"è­¦å‘Š {i}: {e}")
            continue

    if skipped > 3:
        print(f"... è·³è¿‡äº†æ€»å…± {skipped} ä¸ªæ ·æœ¬")

    print("å¤„ç†æµ‹è¯•é›†...")
    test_graphs = []
    skipped = 0
    for i, sample in enumerate(test_dataset):
        try:
            graph = graph_builder.build_graph(sample['text'], sample['aspects'])
            test_graphs.append(graph)
        except Exception as e:
            skipped += 1
            continue

    print(f"è®­ç»ƒå›¾æ•°é‡: {len(train_graphs)}, æµ‹è¯•å›¾æ•°é‡: {len(test_graphs)}")

    if len(train_graphs) == 0 or len(test_graphs) == 0:
        print("é”™è¯¯: æ²¡æœ‰æˆåŠŸæ„å»ºä»»ä½•å›¾ï¼")
        return

    # 4. åˆå§‹åŒ–æ¨¡å‹
    print("\n=== åˆå§‹åŒ–Causal-HAFEæ¨¡å‹ ===")
    if args.model == 'causal_hafe':
        model = CausalHAFE_Model(
            input_dim=768,
            hidden_dim=args.hidden_dim,
            causal_dim=args.causal_dim,
            spurious_dim=args.spurious_dim,
            num_classes=3,
            device=device,
            dataset_name=args.dataset,
            num_frequency_buckets=args.num_frequency_buckets,
            num_confounders=args.num_confounders,
            num_edge_types=4,
            gat_heads=args.gat_heads,
            lambda_indep=args.lambda_indep,
            lambda_bias=args.lambda_bias,
            lambda_ib=args.lambda_ib,
            dropout=args.dropout
        )
        model_name = "Causal-HAFE"
        print(f"æ¨¡å‹: {model_name}")
        print(f"  å› æœç»´åº¦: {args.causal_dim}")
        print(f"  è™šå‡ç»´åº¦: {args.spurious_dim}")
        print(f"  æ··æ·†å› å­æ•°: {args.num_confounders}")
        print(f"  GATå¤´æ•°: {args.gat_heads}")

        # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡ï¼ˆå¿…é¡»åœ¨é¢„å¤„ç†å‰ï¼‰
        model = model.to(device)

        # F3é¢„å¤„ç†
        print("\nF3é¢„å¤„ç†...")
        model.preprocess_f3(train_graphs)

        # åˆå§‹åŒ–æ··æ·†å› å­åŸå‹
        print("\nåˆå§‹åŒ–æ··æ·†å› å­...")
        model.initialize_confounders(train_graphs)

    else:  # baseline
        model = CausalHAFE_Baseline(
            input_dim=768,
            hidden_dim=args.hidden_dim,
            num_classes=3,
            num_edge_types=4,
            dropout=args.dropout
        )
        model_name = "Causal-HAFE Baseline (æ— å› æœæ¨¡å—)"
        print(f"æ¨¡å‹: {model_name}")

        # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
        model = model.to(device)

    # 5. è®­ç»ƒè®¾ç½®
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆé˜²æ­¢è®­ç»ƒä¸ç¨³å®šï¼‰
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=5, verbose=True
    )

    evaluator = ABSAEvaluator()

    best_macro_f1 = 0
    best_metrics = None

    # 6. è®­ç»ƒå¾ªç¯
    print("\n=== å¼€å§‹è®­ç»ƒ ===")
    epoch_start_time = time.time()

    for epoch in range(args.epochs):
        # è®­ç»ƒä¸€ä¸ªepoch
        if args.model == 'causal_hafe':
            train_loss, loss_breakdown = train_epoch_causal(
                model, train_graphs, frequency_buckets, train_dataset, optimizer, device
            )
        else:
            # Baselineä½¿ç”¨æ ‡å‡†è®­ç»ƒ
            from train import train_epoch
            criterion = nn.CrossEntropyLoss()
            train_loss = train_epoch(model, train_graphs, optimizer, criterion, device)
            loss_breakdown = None

        # è®°å½•è®­ç»ƒæ—¥å¿—
        epoch_time = time.time() - epoch_start_time
        current_lr = optimizer.param_groups[0]['lr']

        logger.log_train_step(
            epoch=epoch,
            loss_dict=loss_breakdown or {'total': train_loss},
            lr=current_lr,
            time_elapsed=epoch_time
        )

        if (epoch + 1) % args.eval_every == 0:
            # è¯„ä¼° (è®­ç»ƒæ—¶ä¸ä½¿ç”¨TIE)
            test_metrics = evaluate_causal(
                model, test_graphs, evaluator, train_dataset.aspect_freq, device,
                use_tie=False
            )

            # è®°å½•è¯„ä¼°æ—¥å¿—
            logger.log_eval_step(epoch, test_metrics)

            print(f"\nEpoch {epoch+1}/{args.epochs}")
            print(f"  Train Loss: {train_loss:.4f}")
            if loss_breakdown:
                print(f"    - Task: {loss_breakdown['task']:.4f}")
                print(f"    - Indep: {loss_breakdown['indep']:.4f}")
                print(f"    - Bias: {loss_breakdown['bias']:.4f}")
                print(f"    - IB: {loss_breakdown['ib']:.4f}")
            print(f"  Test Acc: {test_metrics['accuracy']*100:.2f}%")
            print(f"  Test Macro-F1: {test_metrics['macro_f1']*100:.2f}%")
            if 'gini' in test_metrics:
                print(f"  Gini: {test_metrics['gini']:.4f}")
                print(f"  DP-Aspect: {test_metrics['dp_aspect']:.4f}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = test_metrics['macro_f1'] > best_macro_f1
            if is_best:
                best_macro_f1 = test_metrics['macro_f1']
                best_metrics = test_metrics

            # ä¿å­˜æ¨¡å‹checkpoint
            logger.save_model(
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                epoch=epoch,
                is_best=is_best
            )

            if is_best:
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {best_macro_f1:.4f})")

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(test_metrics['macro_f1'])

        epoch_start_time = time.time()

    # 7. æœ€ç»ˆè¯„ä¼° (ä½¿ç”¨TIE)
    if args.model == 'causal_hafe' and args.use_tie_inference:
        print("\n=== ä½¿ç”¨TIEè¿›è¡Œæœ€ç»ˆè¯„ä¼° ===")

        # åŠ è½½æœ€ä½³æ¨¡å‹
        model.load_state_dict(torch.load(f'./checkpoints/{args.dataset}_{args.model}_best.pt'))

        for tie_mode in ['basic', 'adaptive']:
            print(f"\nTIEæ¨¡å¼: {tie_mode}")
            tie_metrics = evaluate_causal(
                model, test_graphs, evaluator, train_dataset.aspect_freq, device,
                use_tie=True, tie_mode=tie_mode
            )

            print(f"  Accuracy: {tie_metrics['accuracy']*100:.2f}%")
            print(f"  Macro-F1: {tie_metrics['macro_f1']*100:.2f}%")
            if 'dp_aspect' in tie_metrics:
                print(f"  DP-Aspect: {tie_metrics['dp_aspect']:.4f}")

            # æ›´æ–°æœ€ä½³ç»“æœå¦‚æœTIEæ›´å¥½
            if tie_metrics['macro_f1'] > best_macro_f1:
                best_macro_f1 = tie_metrics['macro_f1']
                best_metrics = tie_metrics
                print(f"  *** TIEæå‡äº†æ€§èƒ½! ***")

    # 8. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå’Œå¯è§†åŒ–
    total_time = time.time() - start_time
    print(f"\nè®­ç»ƒæ€»ç”¨æ—¶: {total_time:.2f}ç§’ ({total_time/3600:.2f}å°æ—¶)")

    # ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾
    logger.plot_training_curves()

    # ç”Ÿæˆå®éªŒæŠ¥å‘Š
    report = logger.generate_report()

    # æ‰“å°è¯¦ç»†çš„å®éªŒæ€»ç»“
    logger.print_summary()

    # 9. æœ€ç»ˆç»“æœ
    if best_metrics is None:
        print("è®­ç»ƒå¤±è´¥ï¼Œæ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ")
        return

    print("\n" + "="*60)
    print(f"æœ€ç»ˆç»“æœ - {args.dataset.upper()} - {model_name}")
    print("="*60)
    print(f"Accuracy: {best_metrics['accuracy']*100:.2f}%")
    print(f"Macro-F1: {best_metrics['macro_f1']*100:.2f}%")
    print(f"Micro-F1: {best_metrics['micro_f1']*100:.2f}%")

    if 'gini' in best_metrics:
        print(f"\nå…¬å¹³æ€§æŒ‡æ ‡:")
        print(f"  Variance: {best_metrics['variance']:.4f}")
        print(f"  Gap: {best_metrics['gap']:.4f}")
        print(f"  Gini: {best_metrics['gini']:.4f}")
        print(f"  DP-Aspect: {best_metrics['dp_aspect']:.4f}")
        print(f"  High-Freq F1: {best_metrics['high_freq_f1']:.4f}")
        print(f"  Low-Freq F1: {best_metrics['low_freq_f1']:.4f}")

    # 10. Per-aspectè¯¦ç»†ç»“æœ
    if len(best_metrics['per_aspect_f1']) > 0:
        print(f"\nPer-Aspect F1 (Top 10):")
        sorted_aspects = sorted(best_metrics['per_aspect_f1'].items(),
                               key=lambda x: train_dataset.aspect_freq.get(x[0], 0),
                               reverse=True)
        for aspect, f1 in sorted_aspects[:10]:
            freq = train_dataset.aspect_freq.get(aspect, 0)
            print(f"  {aspect:35s} (freq={freq:4d}): F1={f1:.4f}")

    print("="*60)
    print(f"\nå®éªŒç»“æœå·²ä¿å­˜åˆ°: {logger.experiment_dir}")
    print("åŒ…å«: è®­ç»ƒæ—¥å¿—ã€æ¨¡å‹checkpointã€å¯è§†åŒ–å›¾è¡¨ã€è¯¦ç»†æŠ¥å‘Š")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Causal-HAFEè®­ç»ƒè„šæœ¬')

    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--dataset', type=str, default='semeval2014',
                       choices=['semeval2014', 'semeval2016'],
                       help='é€‰æ‹©æ•°æ®é›†')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', type=str, default='causal_hafe',
                       choices=['causal_hafe', 'baseline'],
                       help='é€‰æ‹©æ¨¡å‹')
    parser.add_argument('--hidden_dim', type=int, default=128,
                       help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--causal_dim', type=int, default=128,
                       help='å› æœè¡¨ç¤ºç»´åº¦')
    parser.add_argument('--spurious_dim', type=int, default=64,
                       help='è™šå‡è¡¨ç¤ºç»´åº¦')
    parser.add_argument('--num_confounders', type=int, default=5,
                       help='æ··æ·†å› å­åŸå‹æ•°é‡')
    parser.add_argument('--gat_heads', type=int, default=1,
                       help='GATæ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--dropout', type=float, default=0.3,
                       help='Dropoutæ¯”ä¾‹')

    # DIBæŸå¤±æƒé‡ (é™ä½ä»¥é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
    parser.add_argument('--lambda_indep', type=float, default=0.01,
                       help='è§£è€¦çº¦æŸæƒé‡')
    parser.add_argument('--lambda_bias', type=float, default=0.1,
                       help='åå·®æ‹Ÿåˆæƒé‡')
    parser.add_argument('--lambda_ib', type=float, default=0.001,
                       help='ä¿¡æ¯ç“¶é¢ˆæƒé‡')

    # é¢‘ç‡åˆ†æ¡¶
    parser.add_argument('--num_frequency_buckets', type=int, default=5,
                       help='é¢‘ç‡åˆ†æ¡¶æ•°é‡')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--lr', type=float, default=0.0001,
                       help='å­¦ä¹ ç‡ï¼ˆé™ä½ä»¥æé«˜ç¨³å®šæ€§ï¼‰')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--epochs', type=int, default=50,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--eval_every', type=int, default=5,
                       help='æ¯éš”å¤šå°‘è½®è¯„ä¼°ä¸€æ¬¡')

    # æ¨ç†å‚æ•°
    parser.add_argument('--use_tie_inference', action='store_true',
                       help='åœ¨æœ€ç»ˆè¯„ä¼°æ—¶ä½¿ç”¨TIEåäº‹å®æ¨ç†')

    args = parser.parse_args()

    # æ‰“å°é…ç½®
    print("="*60)
    print("è®­ç»ƒé…ç½®:")
    print("="*60)
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  æ¨¡å‹: {args.model}")
    print(f"  éšè—ç»´åº¦: {args.hidden_dim}")
    print(f"  å› æœç»´åº¦: {args.causal_dim}")
    print(f"  è™šå‡ç»´åº¦: {args.spurious_dim}")
    print(f"  æ··æ·†å› å­æ•°: {args.num_confounders}")
    print(f"  å­¦ä¹ ç‡: {args.lr}")
    print(f"  è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  ä½¿ç”¨TIEæ¨ç†: {'æ˜¯' if args.use_tie_inference else 'å¦'}")
    print("="*60)

    main(args)
