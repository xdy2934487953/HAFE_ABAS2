"""
ç®€åŒ–ç‰ˆCausal-HAFEè®­ç»ƒè„šæœ¬
æ€§èƒ½ä¼˜åŒ–ç‰ˆï¼Œé‡‡ç”¨ç®€åŒ–çš„æ¶æ„å’Œæ›´å¥½çš„è®­ç»ƒç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.optim as optim
import argparse
import os
import sys
import numpy as np

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append('./src')

from data_loader import ABSADataset, download_datasets
from graph_builder import ABSAGraphBuilder
from simplified_causal_hafe import SimplifiedCausalHAFE, SimplifiedHAFE_Baseline
from evaluator import ABSAEvaluator
from utils import ExperimentLogger, create_experiment_logger
import time


def train_epoch_simplified(model, graphs, frequency_buckets, dataset, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch (ç®€åŒ–ç‰ˆCausal-HAFE)"""
    model.train()
    total_loss = 0
    loss_breakdown = {'total': 0, 'task': 0, 'indep': 0, 'bias': 0, 'ib': 0}
    num_samples = 0

    for graph in graphs:
        if graph['labels'].shape[0] == 0:
            continue

        features = graph['features'].to(device)
        edge_index = graph['edge_index'].to(device)
        edge_types = graph.get('edge_types', None)
        if edge_types is not None:
            edge_types = edge_types.to(device)
        node_types = graph.get('node_types', None)  # æ–°å¢ï¼
        if node_types is not None:
            node_types = node_types.to(device)
        positions = graph.get('positions', None)  # æ–°å¢ï¼
        if positions is not None:
            positions = positions.to(device)
        aspect_indices = graph['aspect_indices'].to(device)
        labels = graph['labels'].to(device)

        # è·å–é¢‘ç‡æ ‡ç­¾
        aspect_words = graph['aspect_words']
        frequency_labels = []
        for aspect_word in aspect_words:
            aspect_key = aspect_word.lower()
            freq_label = frequency_buckets.get(aspect_key, 2)  # é»˜è®¤ä¸­é¢‘
            frequency_labels.append(freq_label)
        frequency_labels = torch.tensor(frequency_labels, dtype=torch.long).to(device)

        optimizer.zero_grad()

        try:
            # å‰å‘ä¼ æ’­
            logits, dib_losses = model(
                features, edge_index, aspect_indices, edge_types,
                node_types, positions, frequency_labels, return_dib_losses=True
            )

            # æ£€æŸ¥nan
            if torch.isnan(logits).any():
                print(f"è­¦å‘Š: logitsåŒ…å«nanï¼Œè·³è¿‡è¯¥batch")
                continue

            # è®¡ç®—æ€»æŸå¤±
            loss, loss_dict = model.compute_total_loss(logits, labels, dib_losses)

            # æ£€æŸ¥nan/inf
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"è­¦å‘Š: lossä¸ºnan/infï¼Œè·³è¿‡è¯¥batch")
                continue

            loss.backward()

            # æ¢¯åº¦å‰ªåˆ‡ (æ›´æ¸©å’Œ)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)

            optimizer.step()

            total_loss += loss.item()
            for key in loss_breakdown.keys():
                loss_breakdown[key] += loss_dict[key]
            num_samples += 1

        except Exception as e:
            print(f"è®­ç»ƒé”™è¯¯: {e}")
            continue

    # å¹³å‡æŸå¤±
    if num_samples == 0:
        print("è­¦å‘Š: æ²¡æœ‰æˆåŠŸè®­ç»ƒä»»ä½•æ ·æœ¬ï¼")
        return float('nan'), loss_breakdown

    avg_loss = total_loss / num_samples
    for key in loss_breakdown.keys():
        loss_breakdown[key] /= num_samples

    return avg_loss, loss_breakdown


def evaluate_simplified(model, graphs, evaluator, aspect_freq, device, use_tie=False, tie_mode='basic'):
    """è¯„ä¼°ç®€åŒ–ç‰ˆæ¨¡å‹"""
    model.eval()
    evaluator.reset()

    with torch.no_grad():
        for graph in graphs:
            if graph['labels'].shape[0] == 0:
                continue

            features = graph['features'].to(device)
            edge_index = graph['edge_index'].to(device)
            edge_types = graph.get('edge_types', None)
            if edge_types is not None:
                edge_types = edge_types.to(device)
            node_types = graph.get('node_types', None)  # æ–°å¢ï¼
            if node_types is not None:
                node_types = node_types.to(device)
            positions = graph.get('positions', None)  # æ–°å¢ï¼
            if positions is not None:
                positions = positions.to(device)
            aspect_indices = graph['aspect_indices'].to(device)
            labels = graph['labels'].to(device)

            try:
                if use_tie:
                    # åŸºç¡€æ¨ç† (ç®€åŒ–ç‰ˆä¸æ”¯æŒå¤æ‚TIE)
                    logits = model(features, edge_index, aspect_indices, edge_types, node_types, positions)
                    preds = torch.argmax(logits, dim=1)
                else:
                    # æ ‡å‡†å‰å‘ä¼ æ’­
                    logits = model(features, edge_index, aspect_indices, edge_types, node_types, positions)
                    preds = torch.argmax(logits, dim=1)

                aspects = graph['aspect_words']
                evaluator.add_batch(preds, labels, aspects)

            except Exception as e:
                print(f"è¯„ä¼°é”™è¯¯: {e}")
                continue

    metrics = evaluator.compute_metrics(aspect_freq)
    return metrics


def main(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºå®éªŒæ—¥å¿—è®°å½•å™¨
    experiment_name = f"{args.model}_{args.dataset}_lr{args.lr}_h{args.hidden_dim}"
    logger = create_experiment_logger(
        experiment_name,
        config=vars(args)
    )
    print(f"å®éªŒæ—¥å¿—ç›®å½•: {logger.experiment_dir}")

    start_time = time.time()

    # 1. æ£€æŸ¥æ•°æ®é›†
    if args.dataset == 'semeval2014':
        data_path = './data/semeval2014/'
        train_file = 'Restaurants_Train.xml'
    elif args.dataset == 'semeval2016':
        data_path = './data/semeval2016/'
        train_file = 'ABSA16_Restaurants_Train_SB1_v2.xml'
    else:
        raise ValueError(f"Unknown dataset: {args.dataset}")

    if not os.path.exists(os.path.join(data_path, train_file)):
        print(f"æ•°æ®é›†æœªæ‰¾åˆ°: {args.dataset}")
        download_datasets()
        return

    # 2. åŠ è½½æ•°æ®
    print(f"\n=== åŠ è½½æ•°æ®é›†: {args.dataset} ===")
    train_dataset = ABSADataset(data_path, dataset_name=args.dataset, phase='train')
    test_dataset = ABSADataset(data_path, dataset_name=args.dataset, phase='test')

    # è®¡ç®—é¢‘ç‡åˆ†æ¡¶
    frequency_buckets = train_dataset.compute_frequency_buckets(num_buckets=args.num_frequency_buckets)
    print(f"\né¢‘ç‡åˆ†æ¡¶å®Œæˆ (å…±{args.num_frequency_buckets}ä¸ªæ¡¶)")

    # 3. æ„å»ºå›¾
    print("\n=== æ„å»ºå¢å¼ºå›¾ ===")
    graph_builder = ABSAGraphBuilder(device=device)

    print("å¤„ç†è®­ç»ƒé›†...")
    train_graphs = []
    skipped = 0
    for i, sample in enumerate(train_dataset):
        try:
            graph = graph_builder.build_graph(sample['text'], sample['aspects'])
            train_graphs.append(graph)
        except Exception as e:
            skipped += 1
            continue

    print("å¤„ç†æµ‹è¯•é›†...")
    test_graphs = []
    skipped = 0
    for i, sample in enumerate(test_dataset):
        try:
            graph = graph_builder.build_graph(sample['text'], sample['aspects'])
            test_graphs.append(graph)
        except Exception as e:
            skipped += 1
            continue

    print(f"è®­ç»ƒå›¾æ•°é‡: {len(train_graphs)}, æµ‹è¯•å›¾æ•°é‡: {len(test_graphs)}")

    if len(train_graphs) == 0 or len(test_graphs) == 0:
        print("é”™è¯¯: æ²¡æœ‰æˆåŠŸæ„å»ºä»»ä½•å›¾ï¼")
        return

    # 4. åˆå§‹åŒ–æ¨¡å‹
    print("\n=== åˆå§‹åŒ–ç®€åŒ–ç‰ˆCausal-HAFEæ¨¡å‹ ===")
    if args.model == 'simplified_causal_hafe':
        model = SimplifiedCausalHAFE(
            input_dim=768,
            hidden_dim=args.hidden_dim,
            causal_dim=args.causal_dim,
            spurious_dim=args.spurious_dim,
            num_classes=3,
            device=device,
            dataset_name=args.dataset,
            num_frequency_buckets=args.num_frequency_buckets,
            num_edge_types=4,
            gat_heads=args.gat_heads,
            lambda_indep=args.lambda_indep,
            lambda_bias=args.lambda_bias,
            lambda_ib=args.lambda_ib,
            dropout=args.dropout
        )
        model_name = "Simplified Causal-HAFE"
        print(f"æ¨¡å‹: {model_name}")
        print(f"  éšè—ç»´åº¦: {args.hidden_dim}")
        print(f"  å› æœç»´åº¦: {args.causal_dim}")
        print(f"  è™šå‡ç»´åº¦: {args.spurious_dim}")
        print(f"  GATå¤´æ•°: {args.gat_heads}")

    else:  # baseline
        model = SimplifiedHAFE_Baseline(
            input_dim=768,
            hidden_dim=args.hidden_dim,
            num_classes=3,
            num_edge_types=4,
            gat_heads=args.gat_heads,
            dropout=args.dropout
        )
        model_name = "Simplified HAFE Baseline"
        print(f"æ¨¡å‹: {model_name}")

    # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
    model = model.to(device)

    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # 5. è®­ç»ƒè®¾ç½®
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ (æ›´æ¿€è¿›)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=3, verbose=True, min_lr=1e-6
    )

    evaluator = ABSAEvaluator()

    best_macro_f1 = 0
    best_metrics = None

    # 6. è®­ç»ƒå¾ªç¯
    print("\n=== å¼€å§‹è®­ç»ƒ ===")
    epoch_start_time = time.time()

    for epoch in range(args.epochs):
        # è®­ç»ƒä¸€ä¸ªepoch
        if args.model == 'simplified_causal_hafe':
            train_loss, loss_breakdown = train_epoch_simplified(
                model, train_graphs, frequency_buckets, train_dataset, optimizer, device
            )
        else:
            # Baselineè®­ç»ƒ
            train_loss = train_epoch_baseline(
                model, train_graphs, optimizer, device
            )
            loss_breakdown = None

        # è®°å½•è®­ç»ƒæ—¥å¿—
        epoch_time = time.time() - epoch_start_time
        current_lr = optimizer.param_groups[0]['lr']

        logger.log_train_step(
            epoch=epoch,
            loss_dict=loss_breakdown or {'total': train_loss},
            lr=current_lr,
            time_elapsed=epoch_time
        )

        if (epoch + 1) % args.eval_every == 0:
            # è¯„ä¼°
            test_metrics = evaluate_simplified(
                model, test_graphs, evaluator, train_dataset.aspect_freq, device,
                use_tie=False
            )

            # è®°å½•è¯„ä¼°æ—¥å¿—
            logger.log_eval_step(epoch, test_metrics)

            print(f"\nEpoch {epoch+1}/{args.epochs}")
            print(f"  Train Loss: {train_loss:.4f}")
            if loss_breakdown:
                print(f"    - Task: {loss_breakdown['task']:.4f}")
                print(f"    - Indep: {loss_breakdown['indep']:.4f}")
                print(f"    - Bias: {loss_breakdown['bias']:.4f}")
                print(f"    - IB: {loss_breakdown['ib']:.4f}")
            print(f"  Test Acc: {test_metrics['accuracy']*100:.2f}%")
            print(f"  Test Macro-F1: {test_metrics['macro_f1']*100:.2f}%")
            if 'gini' in test_metrics:
                print(f"  Gini: {test_metrics['gini']:.4f}")
                print(f"  DP-Aspect: {test_metrics['dp_aspect']:.4f}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = test_metrics['macro_f1'] > best_macro_f1
            if is_best:
                best_macro_f1 = test_metrics['macro_f1']
                best_metrics = test_metrics

            # ä¿å­˜æ¨¡å‹checkpoint
            logger.save_model(
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                epoch=epoch,
                is_best=is_best
            )

            if is_best:
                print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {best_macro_f1:.4f})")

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(test_metrics['macro_f1'])

        epoch_start_time = time.time()

    # 7. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå’Œå¯è§†åŒ–
    total_time = time.time() - start_time
    print(f"\nè®­ç»ƒæ€»ç”¨æ—¶: {total_time:.2f}ç§’ ({total_time/3600:.2f}å°æ—¶)")

    # ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾
    logger.plot_training_curves()

    # ç”Ÿæˆå®éªŒæŠ¥å‘Š
    report = logger.generate_report()

    # æ‰“å°è¯¦ç»†çš„å®éªŒæ€»ç»“
    logger.print_summary()

    # 7. æœ€ç»ˆç»“æœ
    if best_metrics is None:
        print("è®­ç»ƒå¤±è´¥ï¼Œæ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ")
        return

    print("\n" + "="*60)
    print(f"æœ€ç»ˆç»“æœ - {args.dataset.upper()} - {model_name}")
    print("="*60)
    print(f"Accuracy: {best_metrics['accuracy']*100:.2f}%")
    print(f"Macro-F1: {best_metrics['macro_f1']*100:.2f}%")
    print(f"Micro-F1: {best_metrics['micro_f1']*100:.2f}%")

    if 'gini' in best_metrics:
        print(f"\nå…¬å¹³æ€§æŒ‡æ ‡:")
        print(f"  Variance: {best_metrics['variance']:.4f}")
        print(f"  Gap: {best_metrics['gap']:.4f}")
        print(f"  Gini: {best_metrics['gini']:.4f}")
        print(f"  DP-Aspect: {best_metrics['dp_aspect']:.4f}")
        print(f"  High-Freq F1: {best_metrics['high_freq_f1']:.4f}")
        print(f"  Low-Freq F1: {best_metrics['low_freq_f1']:.4f}")

    # 8. Per-aspectè¯¦ç»†ç»“æœ
    if len(best_metrics['per_aspect_f1']) > 0:
        print(f"\nPer-Aspect F1 (Top 10):")
        sorted_aspects = sorted(best_metrics['per_aspect_f1'].items(),
                               key=lambda x: train_dataset.aspect_freq.get(x[0], 0),
                               reverse=True)
        for aspect, f1 in sorted_aspects[:10]:
            freq = train_dataset.aspect_freq.get(aspect, 0)
            print(f"  {aspect:35s} (freq={freq:4d}): F1={f1:.4f}")

    print("="*60)
    print(f"\nå®éªŒç»“æœå·²ä¿å­˜åˆ°: {logger.experiment_dir}")
    print("åŒ…å«: è®­ç»ƒæ—¥å¿—ã€æ¨¡å‹checkpointã€å¯è§†åŒ–å›¾è¡¨ã€è¯¦ç»†æŠ¥å‘Š")


def train_epoch_baseline(model, graphs, optimizer, criterion, device):
    """åŸºçº¿æ¨¡å‹è®­ç»ƒ"""
    model.train()
    total_loss = 0
    num_samples = 0

    for graph in graphs:
        if graph['labels'].shape[0] == 0:
            continue

        features = graph['features'].to(device)
        edge_index = graph['edge_index'].to(device)
        edge_types = graph.get('edge_types', None)
        if edge_types is not None:
            edge_types = edge_types.to(device)
        node_types = graph.get('node_types', None)
        if node_types is not None:
            node_types = node_types.to(device)
        positions = graph.get('positions', None)
        if positions is not None:
            positions = positions.to(device)
        aspect_indices = graph['aspect_indices'].to(device)
        labels = graph['labels'].to(device)

        optimizer.zero_grad()

        try:
            logits = model(features, edge_index, aspect_indices, edge_types, node_types, positions)
            loss = criterion(logits, labels)

            if not torch.isnan(loss):
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                optimizer.step()

                total_loss += loss.item()
                num_samples += 1

        except Exception as e:
            continue

    return total_loss / num_samples if num_samples > 0 else float('nan')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ç®€åŒ–ç‰ˆCausal-HAFEè®­ç»ƒè„šæœ¬')

    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--dataset', type=str, default='semeval2014',
                       choices=['semeval2014', 'semeval2016'],
                       help='é€‰æ‹©æ•°æ®é›†')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', type=str, default='simplified_causal_hafe',
                       choices=['simplified_causal_hafe', 'baseline'],
                       help='é€‰æ‹©æ¨¡å‹')
    parser.add_argument('--hidden_dim', type=int, default=256,
                       help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--causal_dim', type=int, default=128,
                       help='å› æœè¡¨ç¤ºç»´åº¦')
    parser.add_argument('--spurious_dim', type=int, default=64,
                       help='è™šå‡è¡¨ç¤ºç»´åº¦')
    parser.add_argument('--gat_heads', type=int, default=4,
                       help='GATæ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--dropout', type=float, default=0.2,
                       help='Dropoutæ¯”ä¾‹')

    # DIBæŸå¤±æƒé‡ (ä¼˜åŒ–åçš„æƒé‡)
    parser.add_argument('--lambda_indep', type=float, default=0.1,
                       help='è§£è€¦çº¦æŸæƒé‡')
    parser.add_argument('--lambda_bias', type=float, default=0.5,
                       help='åå·®æ‹Ÿåˆæƒé‡')
    parser.add_argument('--lambda_ib', type=float, default=0.01,
                       help='ä¿¡æ¯ç“¶é¢ˆæƒé‡')

    # é¢‘ç‡åˆ†æ¡¶
    parser.add_argument('--num_frequency_buckets', type=int, default=5,
                       help='é¢‘ç‡åˆ†æ¡¶æ•°é‡')

    # è®­ç»ƒå‚æ•° (ä¼˜åŒ–åçš„è¶…å‚æ•°)
    parser.add_argument('--lr', type=float, default=0.001,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=5e-5,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--epochs', type=int, default=30,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--eval_every', type=int, default=2,
                       help='æ¯éš”å¤šå°‘è½®è¯„ä¼°ä¸€æ¬¡')

    args = parser.parse_args()

    # æ‰“å°é…ç½®
    print("="*60)
    print("ç®€åŒ–ç‰ˆCausal-HAFEè®­ç»ƒé…ç½®:")
    print("="*60)
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  æ¨¡å‹: {args.model}")
    print(f"  éšè—ç»´åº¦: {args.hidden_dim}")
    print(f"  å› æœç»´åº¦: {args.causal_dim}")
    print(f"  è™šå‡ç»´åº¦: {args.spurious_dim}")
    print(f"  GATå¤´æ•°: {args.gat_heads}")
    print(f"  å­¦ä¹ ç‡: {args.lr}")
    print(f"  è®­ç»ƒè½®æ•°: {args.epochs}")
    print("="*60)

    main(args)
