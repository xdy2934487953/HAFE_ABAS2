# è®­ç»ƒç»“æœä¿å­˜ç³»ç»Ÿ

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

ä¸ºCausal-HAFEå’Œç®€åŒ–ç‰ˆCausal-HAFEæ·»åŠ äº†å®Œæ•´çš„è®­ç»ƒç»“æœä¿å­˜å’Œç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š

- ğŸ“Š **è‡ªåŠ¨æ—¥å¿—è®°å½•**ï¼šè®­ç»ƒè¿‡ç¨‹ã€è¯„ä¼°æŒ‡æ ‡ã€æ¨¡å‹å‚æ•°
- ğŸ’¾ **æ™ºèƒ½æ¨¡å‹ä¿å­˜**ï¼šæœ€ä½³æ¨¡å‹ã€å®šæœŸcheckpointã€æœ€æ–°çŠ¶æ€
- ğŸ“ˆ **å¯è§†åŒ–å›¾è¡¨**ï¼šè®­ç»ƒæ›²çº¿ã€æ€§èƒ½å¯¹æ¯”ã€æ”¶æ•›åˆ†æ
- ğŸ“‹ **å®éªŒç®¡ç†**ï¼šå¤šå®éªŒå¯¹æ¯”ã€æ±‡æ€»æŠ¥å‘Šã€é…ç½®ç®¡ç†
- ğŸ” **è¯¦ç»†åˆ†æ**ï¼šæ”¶æ•›çŠ¶æ€ã€æ€§èƒ½è¶‹åŠ¿ã€è¶…å‚æ•°å½±å“

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è®­ç»ƒæ—¶è‡ªåŠ¨ä¿å­˜

è¿è¡Œè®­ç»ƒè„šæœ¬æ—¶ä¼šè‡ªåŠ¨åˆ›å»ºå®éªŒæ—¥å¿—ï¼š

```bash
# ç®€åŒ–ç‰ˆCausal-HAFE
python train_simplified.py --model simplified_causal_hafe --dataset semeval2014

# åŸç‰ˆCausal-HAFE
python train_causal.py --model causal_hafe --dataset semeval2014
```

è®­ç»ƒå®Œæˆåä¼šæ˜¾ç¤ºï¼š
```
å®éªŒæ—¥å¿—ç›®å½•: ./experiments/simplified_causal_hafe_semeval2014_lr0.001_h256_20241220_143000
```

## ğŸ“ å®éªŒç›®å½•ç»“æ„

æ¯ä¸ªå®éªŒè‡ªåŠ¨åˆ›å»ºå®Œæ•´çš„ç›®å½•ç»“æ„ï¼š

```
experiments/
â””â”€â”€ model_dataset_params_timestamp/
    â”œâ”€â”€ checkpoints/           # æ¨¡å‹checkpoint
    â”‚   â”œâ”€â”€ latest.pt         # æœ€æ–°æ¨¡å‹
    â”‚   â”œâ”€â”€ best.pt          # æœ€ä½³æ¨¡å‹
    â”‚   â””â”€â”€ epoch_10.pt      # å®šæœŸä¿å­˜
    â”œâ”€â”€ logs/                 # è®­ç»ƒæ—¥å¿—
    â”‚   â”œâ”€â”€ train_log.csv    # è®­ç»ƒè¿‡ç¨‹æ—¥å¿—
    â”‚   â””â”€â”€ eval_log.csv     # è¯„ä¼°æŒ‡æ ‡æ—¥å¿—
    â”œâ”€â”€ plots/                # å¯è§†åŒ–å›¾è¡¨
    â”‚   â””â”€â”€ training_curves.png
    â”œâ”€â”€ configs/              # é…ç½®æ–‡ä»¶
    â”‚   â””â”€â”€ experiment_config.json
    â””â”€â”€ experiment_report.json # å®éªŒæ€»ç»“æŠ¥å‘Š
```

## ğŸ“Š æ—¥å¿—å†…å®¹

### è®­ç»ƒæ—¥å¿— (train_log.csv)
| å­—æ®µ | è¯´æ˜ |
|------|------|
| epoch | è®­ç»ƒè½®æ•° |
| train_loss | è®­ç»ƒæŸå¤± |
| task_loss | ä»»åŠ¡æŸå¤± |
| indep_loss | è§£è€¦æŸå¤± |
| bias_loss | åå·®æ‹ŸåˆæŸå¤± |
| ib_loss | ä¿¡æ¯ç“¶é¢ˆæŸå¤± |
| lr | å­¦ä¹ ç‡ |
| grad_norm | æ¢¯åº¦èŒƒæ•° |
| time_elapsed | æ¯è½®è€—æ—¶(ç§’) |

### è¯„ä¼°æ—¥å¿— (eval_log.csv)
| å­—æ®µ | è¯´æ˜ |
|------|------|
| epoch | è¯„ä¼°è½®æ•° |
| accuracy | å‡†ç¡®ç‡ |
| macro_f1 | å®å¹³å‡F1 |
| micro_f1 | å¾®å¹³å‡F1 |
| gini | åŸºå°¼ç³»æ•°(å…¬å¹³æ€§) |
| dp_aspect | æ–¹é¢çº§å·®å¼‚(å…¬å¹³æ€§) |
| high_freq_f1 | é«˜é¢‘æ–¹é¢F1 |
| low_freq_f1 | ä½é¢‘æ–¹é¢F1 |

## ğŸ› ï¸ å®éªŒç®¡ç†å·¥å…·

### åˆ—å‡ºæ‰€æœ‰å®éªŒ
```bash
python experiment_manager.py --action list
```

### åˆ†æå•ä¸ªå®éªŒ
```bash
python experiment_manager.py --action analyze --experiments ./experiments/å®éªŒå
```

### å¯¹æ¯”å¤šä¸ªå®éªŒ
```bash
python experiment_manager.py --action compare \
    --experiments ./experiments/exp1 ./experiments/exp2 ./experiments/exp3 \
    --output comparison_report.json
```

### ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
```bash
python experiment_manager.py --action plot \
    --experiments ./experiments/exp1 ./experiments/exp2 \
    --metrics macro_f1 accuracy gini \
    --output comparison_plot.png
```

### åˆ›å»ºæ±‡æ€»è¡¨æ ¼
```bash
python experiment_manager.py --action table \
    --experiments ./experiments/exp1 ./experiments/exp2 \
    --output experiments_summary.csv
```

## ğŸ“ˆ å¯è§†åŒ–åŠŸèƒ½

### è‡ªåŠ¨ç”Ÿæˆçš„å›¾è¡¨
è®­ç»ƒå®Œæˆåè‡ªåŠ¨ç”Ÿæˆï¼š
- **è®­ç»ƒæŸå¤±æ›²çº¿**ï¼šæ€»æŸå¤±ã€ä»»åŠ¡æŸå¤±ã€è§£è€¦æŸå¤±ç­‰
- **è¯„ä¼°æŒ‡æ ‡æ›²çº¿**ï¼šå‡†ç¡®ç‡ã€F1åˆ†æ•°ã€å…¬å¹³æ€§æŒ‡æ ‡
- **å­¦ä¹ ç‡å’Œæ¢¯åº¦**ï¼šå­¦ä¹ ç‡å˜åŒ–ã€æ¢¯åº¦èŒƒæ•°ç›‘æ§

### å¯¹æ¯”å›¾è¡¨
ä½¿ç”¨experiment_manager.pyç”Ÿæˆï¼š
- **å¤šå®éªŒæ€§èƒ½å¯¹æ¯”**
- **ä¸åŒæŒ‡æ ‡çš„è¶‹åŠ¿å›¾**
- **è¶…å‚æ•°å½±å“åˆ†æ**

## ğŸ“‹ å®éªŒæŠ¥å‘Š

æ¯ä¸ªå®éªŒç”Ÿæˆè¯¦ç»†æŠ¥å‘Š (experiment_report.json)ï¼š

```json
{
  "experiment_name": "simplified_causal_hafe_semeval2014_lr0.001_h256",
  "timestamp": "20241220_143000",
  "config": {...},
  "best_metrics": {
    "epoch": 25,
    "macro_f1": 0.7421,
    "accuracy": 0.8115,
    "gini": 0.2341
  },
  "final_metrics": {...},
  "training_summary": {
    "avg_total_loss": 0.4231,
    "final_total_loss": 0.1123,
    "loss_convergence": "converged"
  }
}
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰å®éªŒåç§°
```python
from utils import ExperimentLogger

logger = ExperimentLogger(
    experiment_name="my_custom_experiment",
    save_dir="./my_experiments",
    config={"custom_param": "value"}
)
```

### æ‰‹åŠ¨è®°å½•è®­ç»ƒæ­¥éª¤
```python
logger.log_train_step(
    epoch=epoch,
    loss_dict={'total': 0.5, 'task': 0.3},
    lr=0.001,
    time_elapsed=45.2
)

logger.log_eval_step(epoch, metrics)
```

### ä¿å­˜æ¨¡å‹checkpoint
```python
logger.save_model(
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    epoch=epoch,
    is_best=True
)
```

### ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
```python
# ç”Ÿæˆå¯è§†åŒ–
logger.plot_training_curves()

# ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
report = logger.generate_report()

# æ‰“å°æ€»ç»“
logger.print_summary()
```

## ğŸ“Š å®éªŒå¯¹æ¯”åˆ†æ

### æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
```
å®éªŒæ±‡æ€»è¡¨æ ¼:
====================================================================================================
Experiment                          Model                  Dataset      LR    Best_Macro_F1  Best_Accuracy
----------------------------------------------------------------------------------------------------
simplified_causal_hafe_semeval2014  simplified_causal_hafe  semeval2014  0.001      0.7421         0.8115
causal_hafe_semeval2014             causal_hafe            semeval2014  0.0001     0.6832         0.7543
baseline_semeval2014                baseline               semeval2014  0.001      0.6987         0.7721
```

### æ”¶æ•›åˆ†æ
- **converged**: æŸå¤±åœ¨æœ€å10è½®ä¸­æ ‡å‡†å·® < 1%
- **converging**: æŸå¤±åœ¨æœ€å10è½®ä¸­æ ‡å‡†å·® < 5%
- **not_converged**: æŸå¤±ä»æœªç¨³å®š

## ğŸ¯ æœ€ä½³å®è·µ

### 1. å®éªŒå‘½å
- ä½¿ç”¨æè¿°æ€§åç§°ï¼š`{model}_{dataset}_{key_params}_{timestamp}`
- åŒ…å«é‡è¦è¶…å‚æ•°ï¼šå­¦ä¹ ç‡ã€éšè—ç»´åº¦ç­‰

### 2. å®šæœŸæ¸…ç†
```bash
# åªä¿ç•™æœ€ä½³å®éªŒ
find ./experiments -name "*best.pt" -exec dirname {} \; | sort -u

# åˆ é™¤æ—§å®éªŒ
find ./experiments -mtime +30 -type d -exec rm -rf {} \;
```

### 3. æ‰¹é‡åˆ†æ
```bash
# åˆ†ææ‰€æœ‰å®éªŒ
python experiment_manager.py --action analyze

# ç”Ÿæˆå®Œæ•´å¯¹æ¯”æŠ¥å‘Š
python experiment_manager.py --action table --output all_experiments.csv
python experiment_manager.py --action plot --output all_comparison.png
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: å®éªŒç›®å½•æ²¡æœ‰åˆ›å»ºï¼Ÿ**
A: æ£€æŸ¥å†™å…¥æƒé™ï¼Œç¡®ä¿`./experiments`ç›®å½•å­˜åœ¨ä¸”å¯å†™ã€‚

**Q: æ—¥å¿—æ–‡ä»¶æŸåï¼Ÿ**
A: åˆ é™¤æŸåçš„å®éªŒç›®å½•ï¼Œé‡æ–°è¿è¡Œè®­ç»ƒã€‚

**Q: å¯è§†åŒ–å›¾è¡¨ä¸ºç©ºï¼Ÿ**
A: ç¡®ä¿å®‰è£…äº†matplotlibå’Œseabornï¼š
```bash
pip install matplotlib seaborn
```

**Q: å†…å­˜ä¸è¶³ï¼Ÿ**
A: å‡å°‘`--eval_every`å‚æ•°ï¼Œæˆ–åœ¨åˆ†ææ—¶åªåŠ è½½å¿…è¦çš„å®éªŒã€‚

## ğŸ“š APIå‚è€ƒ

### ExperimentLogger
```python
class ExperimentLogger:
    def __init__(experiment_name, save_dir="./experiments", config=None)
    def log_train_step(epoch, loss_dict, lr=None, grad_norm=None, time_elapsed=None)
    def log_eval_step(epoch, metrics)
    def save_model(model, optimizer=None, scheduler=None, epoch=None, is_best=False)
    def load_model(model, checkpoint_path, optimizer=None, scheduler=None)
    def plot_training_curves(save_plots=True)
    def generate_report()
    def print_summary()
```

### ABSAResultsManager
```python
class ABSAResultsManager:
    def __init__(results_dir="./experiments")
    def load_experiment(exp_dir)
    def compare_experiments(exp_names, metrics=['macro_f1', 'accuracy'])
    def generate_comparison_report(exp_names, output_file=None)
```

è¿™ä¸ªè®­ç»ƒç»“æœä¿å­˜ç³»ç»Ÿè®©æ‚¨èƒ½å¤Ÿï¼š
- ğŸ”„ **é‡ç°å®éªŒ**ï¼šå®Œæ•´çš„é…ç½®å’Œcheckpoint
- ğŸ“Š **åˆ†ææ€§èƒ½**ï¼šè¯¦ç»†çš„æŒ‡æ ‡è¿½è¸ªå’Œå¯è§†åŒ–
- ğŸ” **å¯¹æ¯”å®éªŒ**ï¼šç³»ç»Ÿæ€§çš„å¤šå®éªŒæ€§èƒ½å¯¹æ¯”
- ğŸ“ˆ **ä¼˜åŒ–å‚æ•°**ï¼šåŸºäºå†å²æ•°æ®çš„è¶…å‚æ•°è°ƒä¼˜
